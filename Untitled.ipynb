{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "import nltk\n",
    "from train import get_instance\n",
    "from utils.util import get_caption_lengths\n",
    "from model.damsm import DAMSM_RNN_Encoder\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = 'output/hdgan/HDGAN-Flowers/0321_132734/0321_132734/checkpoint-epoch78.pth'\n",
    "config = torch.load(resume, map_location='cpu')['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29390/29390 [00:42<00:00, 686.32it/s]\n",
      "100%|██████████| 5775/5775 [00:08<00:00, 717.61it/s]\n"
     ]
    }
   ],
   "source": [
    "data_loader = getattr(module_data, config['train_data_loader']['type'])(\n",
    "    config['train_data_loader']['args']['data_dir'],\n",
    "    config['train_data_loader']['args']['dataset_name'],\n",
    "    which_set='test',\n",
    "    batch_size=1,\n",
    "    image_size=256,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Init HDGAN Generator\n",
      "\t side output at [64, 128, 256]\n"
     ]
    }
   ],
   "source": [
    "model_config = config[\"models\"]\n",
    "# initialize generator and discriminator from config\n",
    "# build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler\n",
    "model = get_instance(module_arch, 'Generator', model_config)\n",
    "model.eval()\n",
    "\n",
    "# load state dict\n",
    "checkpoint = torch.load(resume, map_location='cpu')\n",
    "state_dict = checkpoint['generator_state_dict']\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load damsm ecoding model\n"
     ]
    }
   ],
   "source": [
    "dirname = '/Users/leon/Projects/I2T2I/'\n",
    "flowers_damsm = os.path.join(dirname, 'output/Deep-Attentional-Multimodal-Similarity-Flowers/0316_120632/model_best.pth')\n",
    "\n",
    "print(\"load damsm ecoding model\")\n",
    "damsm_rnn_encoder = DAMSM_RNN_Encoder(\n",
    "         vocab_size=len(data_loader.dataset.vocab),\n",
    "         word_embed_size=256,\n",
    "         lstm_hidden_size=256)\n",
    "\n",
    "damsm_rnn_encoder = torch.nn.DataParallel(damsm_rnn_encoder, device_ids=[0])\n",
    "\n",
    "if \"Bird\" in config[\"name\"]:\n",
    "    resume_path = birds_damsm\n",
    "elif \"Flower\" in config[\"name\"]:\n",
    "    resume_path = flowers_damsm\n",
    "elif \"CoCo\" in config[\"name\"]:\n",
    "    resume_path = coco_damsm\n",
    "else:\n",
    "    raise ValueError(\"cannot find corresponding damsm model path\")\n",
    "checkpoint = torch.load(resume_path, map_location='cpu')\n",
    "\n",
    "damsm_rnn_encoder.load_state_dict(checkpoint[\"rnn_state_dict\"])\n",
    "for p in damsm_rnn_encoder.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'this flower has round red petals that has purple stamen'\n",
    "# sentence = 'the flower shown haswhite petals along with yellow anther'\n",
    "tokens = nltk.tokenize.word_tokenize(str(sentence).lower())\n",
    "caption = []\n",
    "caption.append(data_loader.dataset.vocab(data_loader.dataset.vocab.start_word))\n",
    "caption.extend(data_loader.dataset.vocab(token) for token in tokens)\n",
    "caption.append(data_loader.dataset.vocab(data_loader.dataset.vocab.end_word))\n",
    "caption, caption_length = get_caption_lengths([caption])\n",
    "_, right_embeddings = damsm_rnn_encoder(caption, caption_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    noise = Variable(torch.randn(1, 100))\n",
    "    noise = noise.view(noise.size(0), 100)\n",
    "    _,_,images,_,_ = model(right_embeddings, noise)\n",
    "\n",
    "    mean = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)\n",
    "    std = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=(-mean / std).tolist(), std=(1.0 / std).tolist()),\n",
    "    transforms.ToPILImage()]\n",
    "    )\n",
    "\n",
    "    pil_image = transform(images[0])\n",
    "\n",
    "    pil_image.save('flowers/{}.png'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
